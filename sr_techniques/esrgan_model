import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import tacoreader
import rasterio as rio
import numpy as np
import random
import matplotlib.pyplot as plt
import math
import os
from torchvision.models import squeezenet1_0, SqueezeNet1_0_Weights

# ---------- 1. Dataset (color) ----------
class Sen2NaipDataset(Dataset):
    def __init__(self, split="tacofoundation:sen2naipv2-unet", indices=None, patch_size=64, scale=4):
        self.dataset = tacoreader.load(split)
        self.indices = indices if indices else range(len(self.dataset))
        self.patch_size = patch_size
        self.scale = scale
        self.hr_patch_size = patch_size * scale

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        sample_idx = self.indices[idx]
        lr_ref = self.dataset.read(sample_idx).read(0)
        hr_ref = self.dataset.read(sample_idx).read(1)

        with rio.open(lr_ref) as src_lr, rio.open(hr_ref) as src_hr:
            lr_data = src_lr.read(window=rio.windows.Window(0, 0, self.patch_size, self.patch_size))
            hr_data = src_hr.read(window=rio.windows.Window(0, 0, self.hr_patch_size, self.hr_patch_size))

        return self.to_tensor_color(lr_data), self.to_tensor_color(hr_data)

    def to_tensor_color(self, data, norm=3000.0):
        # normalize and convert to float tensor with shape [3, H, W]
        rgb = data[:3] / norm
        return torch.tensor(rgb, dtype=torch.float32)

# ---------- 2. ESRGAN Generator (color input/output) ----------
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.block = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(channels),
            nn.PReLU(),
            nn.Conv2d(channels, channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(channels)
        )

    def forward(self, x):
        return x + self.block(x)

class ESRGANGenerator(nn.Module):
    def __init__(self, num_res_blocks=16):
        super(ESRGANGenerator, self).__init__()
        self.initial = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=9, padding=4),  # 3 channels now
            nn.PReLU()
        )

        self.residuals = nn.Sequential(*[ResidualBlock(64) for _ in range(num_res_blocks)])

        self.mid = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64)
        )

        self.upsample = nn.Sequential(
            nn.Conv2d(64, 256, kernel_size=3, padding=1),
            nn.PixelShuffle(2),
            nn.PReLU(),
            nn.Conv2d(64, 256, kernel_size=3, padding=1),
            nn.PixelShuffle(2),
            nn.PReLU()
        )

        self.final = nn.Conv2d(64, 3, kernel_size=9, padding=4)  # 3 channels output

    def forward(self, x):
        initial = self.initial(x)
        res = self.residuals(initial)
        mid = self.mid(res)
        combined = initial + mid
        upsampled = self.upsample(combined)
        return self.final(upsampled)

# ---------- 3. Perceptual Loss with SqueezeNet ----------
class PerceptualLoss(nn.Module):
    def __init__(self, device):
        super(PerceptualLoss, self).__init__()
        weights = SqueezeNet1_0_Weights.DEFAULT
        model = squeezenet1_0(weights=weights).features[:5]  # first few layers
        self.features = model.to(device)
        self.features.eval()
        for param in self.features.parameters():
            param.requires_grad = False

    def forward(self, x, y):
        return F.l1_loss(self.features(x), self.features(y))

# ---------- 4. Training ----------
def compute_psnr(output, target, max_pixel=1.0):
    mse = torch.mean((output - target) ** 2)
    return 100 if mse == 0 else 20 * math.log10(max_pixel / math.sqrt(mse))

def train_esrgan(model, dataloader, device, num_epochs=5, save_path="esrgan_epoch"):
    print("Begin ESRGAN Training")
    criterion = nn.L1Loss()
    perceptual_criterion = PerceptualLoss(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    psnr_list = []
    perceptual_loss_list = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        running_perceptual_loss = 0.0

        for i, (lr, hr) in enumerate(dataloader):
            lr, hr = lr.to(device), hr.to(device)
            optimizer.zero_grad()
            output = model(lr)

            l1_loss = criterion(output, hr)
            perceptual_loss = perceptual_criterion(output, hr)
            loss = l1_loss + 0.01 * perceptual_loss
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            running_perceptual_loss += perceptual_loss.item()

            if (i + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/10:.4f}")
                running_loss = 0.0

        # Calculate PSNR on last batch output/target
        psnr = compute_psnr(output, hr)
        print(f"Epoch [{epoch+1}], PSNR: {psnr:.2f} dB, Perceptual Loss: {running_perceptual_loss/(i+1):.6f}")

        psnr_list.append(psnr)
        perceptual_loss_list.append(running_perceptual_loss / (i+1))

        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), f"{save_path}_{epoch+1}.pth")
            print(f"Model saved: {save_path}_{epoch+1}.pth")

    # Plot PSNR and perceptual loss over epochs
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    plt.plot(range(1, num_epochs+1), psnr_list, marker='o')
    plt.title("PSNR over epochs")
    plt.xlabel("Epoch")
    plt.ylabel("PSNR (dB)")

    plt.subplot(1,2,2)
    plt.plot(range(1, num_epochs+1), perceptual_loss_list, marker='o', color='orange')
    plt.title("Perceptual Loss over epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Perceptual Loss")

    plt.tight_layout()
    plt.show()

# ---------- 5. Inference ----------
def test_esrgan(model, dataset, device):
    model.eval()
    idx = random.randint(0, len(dataset) - 1)
    lr, hr = dataset[idx]
    lr = lr.unsqueeze(0).to(device)
    hr = hr.unsqueeze(0).to(device)

    with torch.no_grad():
        sr = model(lr)

    # Plot
    plt.figure(figsize=(15, 5))
    for i, img in enumerate([lr.squeeze().cpu().numpy().transpose(1,2,0),
                             hr.squeeze().cpu().numpy().transpose(1,2,0),
                             sr.squeeze().cpu().numpy().transpose(1,2,0)]):
        plt.subplot(1, 3, i + 1)
        plt.imshow(np.clip(img, 0, 1))
        plt.title(['LR Input', 'HR Ground Truth', 'SR Output'][i])
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# ---------- 6. Entrypoint ----------
def main(train=True, model_path="esrgan_epoch_5.pth"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ESRGANGenerator().to(device)

    if train:
        train_dataset = Sen2NaipDataset(indices=range(15))  # Adjust size as needed
        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
        train_esrgan(model, train_loader, device)
    else:
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found at {model_path}")
        model.load_state_dict(torch.load(model_path, map_location=device))
        test_dataset = Sen2NaipDataset(indices=range(1))  # Adjust size as needed
        test_esrgan(model, test_dataset, device)

# Run training or testing
# main(train=True)
main(train=False)
